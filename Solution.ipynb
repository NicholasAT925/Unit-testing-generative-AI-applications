{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Stuff"
      ],
      "metadata": {
        "id": "3z6vpvj3Mbze"
      },
      "id": "3z6vpvj3Mbze"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will start the process of allocating a runtime for you. This may take few minutes to fully initialize the runtime."
      ],
      "metadata": {
        "id": "kST0XEqJMkdg"
      },
      "id": "kST0XEqJMkdg"
    },
    {
      "cell_type": "code",
      "id": "0vA10aSR4R01OENAIkq7QXPX",
      "metadata": {
        "tags": [],
        "id": "0vA10aSR4R01OENAIkq7QXPX"
      },
      "source": [
        "from IPython.display import clear_output"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paste the below code into the next code cell and run the cell."
      ],
      "metadata": {
        "id": "PCMm4stDMnSM"
      },
      "id": "PCMm4stDMnSM"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet ipytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwLPk0wPMeaT",
        "outputId": "0f3ddeb5-eb63-498f-af1e-8567e5017c99"
      },
      "id": "pwLPk0wPMeaT",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.6 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = !gcloud config get project\n",
        "project_id = project_id[0]"
      ],
      "metadata": {
        "id": "bdyhwHcTMtRm"
      },
      "id": "bdyhwHcTMtRm",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import packages and run a basic test"
      ],
      "metadata": {
        "id": "MAEOax-bMpFo"
      },
      "id": "MAEOax-bMpFo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, we will begin by importing the necessary packages and running a basic test. This will help verify that the packages are installed correctly and that the basic functionality is working as expected."
      ],
      "metadata": {
        "id": "5kyTn016M0aN"
      },
      "id": "5kyTn016M0aN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "import packages and configure the ipytest package."
      ],
      "metadata": {
        "id": "V88Yid5yM5hr"
      },
      "id": "V88Yid5yM5hr"
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "import pytest\n",
        "import ipytest\n",
        "ipytest.autoconfig()"
      ],
      "metadata": {
        "id": "R_GtcVxMMywd"
      },
      "id": "R_GtcVxMMywd",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTest identifies which functions are tests by looking for any function beginning with test. Within a test, you use an assert statement to test that a value is what you expect it will be."
      ],
      "metadata": {
        "id": "K0o4fRH8NDiu"
      },
      "id": "K0o4fRH8NDiu"
    },
    {
      "cell_type": "code",
      "source": [
        "def test_addition():\n",
        "  assert 2+2 == 4"
      ],
      "metadata": {
        "id": "TuhXxFYLNAl9"
      },
      "id": "TuhXxFYLNAl9",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then run your testing package to run all defined test functions."
      ],
      "metadata": {
        "id": "NBZj0mGkNI8u"
      },
      "id": "NBZj0mGkNI8u"
    },
    {
      "cell_type": "code",
      "source": [
        "ipytest.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-J6r-WtNHCw",
        "outputId": "b3a1f53f-a988-40a1-f3be-fa8e171566c0"
      },
      "id": "p-J6r-WtNHCw",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
            "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ExitCode.OK: 0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tests that have passed are represented by a period. Tests that fail are represented by an “F”. Here your expected output shows one period, representing your basic addition test has passed:"
      ],
      "metadata": {
        "id": "1GIHV1ubNOYK"
      },
      "id": "1GIHV1ubNOYK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write a test for a prompt template"
      ],
      "metadata": {
        "id": "1bweT0DINUNh"
      },
      "id": "1bweT0DINUNh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, we will use the %%writefile cell magic command to save your application's primary content generation prompt template to a file."
      ],
      "metadata": {
        "id": "sSV9g8H1NYYW"
      },
      "id": "sSV9g8H1NYYW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we will deploy a farming question answering bot that can use information you provide as context to answer a user’s question. In a new cell,"
      ],
      "metadata": {
        "id": "UxPK0qzbNanh"
      },
      "id": "UxPK0qzbNanh"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile prompt_template.txt\n",
        "\n",
        "Respond to the user's query.\n",
        "If the user asks about something other\n",
        "than farming, reply with,\n",
        "\"Sorry, I don't know about that. Ask me something about farming instead.\"\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "User Query: {query}\n",
        "Response:"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mbmjRC-NLue",
        "outputId": "69470969-b826-45aa-e084-6041a6cd859b"
      },
      "id": "3mbmjRC-NLue",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing prompt_template.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paste the following and run it (with Shift + Return) to define this fixture for your tests to use the prompt template."
      ],
      "metadata": {
        "id": "dkOkX2W6NjdB"
      },
      "id": "dkOkX2W6NjdB"
    },
    {
      "cell_type": "code",
      "source": [
        "@pytest.fixture\n",
        "def prompt_template():\n",
        "  with open(\"prompt_template.txt\", \"r\") as f:\n",
        "    return f.read()"
      ],
      "metadata": {
        "id": "BlNmCb-sNeHx"
      },
      "id": "BlNmCb-sNeHx",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize VertexAI and models for generation and evaluation"
      ],
      "metadata": {
        "id": "3D_nC1nsNpp8"
      },
      "id": "3D_nC1nsNpp8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, we will initialize VertexAI and configure models for both content generation and evaluation."
      ],
      "metadata": {
        "id": "gsI5JvG_N6rM"
      },
      "id": "gsI5JvG_N6rM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paste this code into a cell and run it to instantiate two models: gen_model for generating responses, which is the model planned for production, and eval_model for evaluating the responses from gen_model. Using two different models ensures that gen_model cannot generate an unusual response and then inaccurately evaluate itself as having given a good response."
      ],
      "metadata": {
        "id": "xCxSPC-eN9HT"
      },
      "id": "xCxSPC-eN9HT"
    },
    {
      "cell_type": "code",
      "source": [
        "vertexai.init(project=project_id, location=\"us-central1\")\n",
        "\n",
        "gen_config = GenerationConfig(\n",
        "    temperature=0,\n",
        "    top_p=0.6,\n",
        "    candidate_count=1,\n",
        "    max_output_tokens=4096,\n",
        ")\n",
        "gen_model = GenerativeModel(\"gemini-2.0-flash\", generation_config=gen_config)\n",
        "\n",
        "eval_config = {\n",
        "        \"temperature\": 0,\n",
        "        \"max_output_tokens\": 1024,\n",
        "        \"top_p\": 0.6,\n",
        "        \"top_k\": 40,\n",
        "    }\n",
        "eval_model = GenerativeModel(\"gemini-2.0-flash\", generation_config=eval_config)"
      ],
      "metadata": {
        "id": "aXLNCIppNmjs"
      },
      "id": "aXLNCIppNmjs",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write a test to generate and evaluate content"
      ],
      "metadata": {
        "id": "PdG19IDyOJdC"
      },
      "id": "PdG19IDyOJdC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, you will write your first LLM-specific test. In the test function below, you will provide specific context, which represents context that you would typically pull from a RAG retrieval system or another external lookup to enhance your model’s response.\n",
        "\n",
        "You will use a known context and a query that you know can be answered from that context.\n",
        "\n",
        "Next, provide an evaluation prompt, clearly giving the evaluation model the expected answer.\n",
        "\n",
        "Our primary gen_model is asked to answer the query given the context using the prompt_template you created earlier. Then, the query and the gen_model's response are passed to the eval_model within the evaluation_prompt to assess if it got the answer correct.\n",
        "\n",
        "The eval_model can evaluate if the substance of the response is correct, even if the generative model has responded with full sentences that may not exactly match a pre-prepared reference answer. You’ll ask the eval_model to respond with a clear ‘yes’ or ‘no’ to assert that the test should pass."
      ],
      "metadata": {
        "id": "8Gj6z4ZuONV1"
      },
      "id": "8Gj6z4ZuONV1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review the code and then paste and run it in a cell to define this test."
      ],
      "metadata": {
        "id": "dKbEUfz5OWZW"
      },
      "id": "dKbEUfz5OWZW"
    },
    {
      "cell_type": "code",
      "source": [
        "def test_basic_response(prompt_template):\n",
        "\n",
        "  context = (\"MightyGo unveiled its 2025 model year Arcturus \"\n",
        "            + \"tractor line at the Salt of the Earth Farm Expo in \"\n",
        "            + \"Málaga in late June.\")\n",
        "\n",
        "  query = \"What is the name of the new tractor model?\"\n",
        "\n",
        "  evaluation_prompt = \"\"\"\n",
        "    Has the query been answered by the provided_response?\n",
        "    The new tractor model is the Arcturus.\n",
        "    Respond with only one word: yes or no\n",
        "\n",
        "    query: {query}\n",
        "    provided_response: {provided_response}\n",
        "    evaluation: \"\"\"\n",
        "\n",
        "  prompt = prompt_template.format(context=context, query=query)\n",
        "\n",
        "  response = gen_model.generate_content(prompt)\n",
        "  print(response.text)\n",
        "  ep = evaluation_prompt.format(query=query, provided_response=response.text)\n",
        "  evaluation = eval_model.generate_content(ep)\n",
        "\n",
        "  assert evaluation.text.strip().lower() == \"yes\""
      ],
      "metadata": {
        "id": "gU5NU4adOFBR"
      },
      "id": "gU5NU4adOFBR",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run your testing framework again, passing the -rP parameter which allows us to see the outputs of tests print statements in your test output.\n",
        "\n",
        "Review the test output, where you should see two tests have passed (indicated by the two periods): your initial example addition test and your new test. The gen_model’s response is printed under the “Captured stdout call” label, allowing you to validate that it indeed looks correct."
      ],
      "metadata": {
        "id": "jUqBjJj0OnmI"
      },
      "id": "jUqBjJj0OnmI"
    },
    {
      "cell_type": "code",
      "source": [
        "ipytest.run('-rP')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7M1ZlzXOkiM",
        "outputId": "4e598f30-fcf3-400d-9f95-35d9d95b1786"
      },
      "id": "F7M1ZlzXOkiM",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                           [100%]\u001b[0m\n",
            "============================================== PASSES ==============================================\n",
            "\u001b[32m\u001b[1m_______________________________________ test_basic_response ________________________________________\u001b[0m\n",
            "--------------------------------------- Captured stdout call ---------------------------------------\n",
            "The new tractor model is called Arcturus.\n",
            "\n",
            "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.72s\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ExitCode.OK: 0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write a test to ensure the model avoids off-topic content"
      ],
      "metadata": {
        "id": "LQMlBGEROydh"
      },
      "id": "LQMlBGEROydh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, Now you’ll write another test to confirm that the model handles off-topic questions and requests as intended. First, let’s confirm that the model knows something about a non-farming topic, such as Frank Sinatra."
      ],
      "metadata": {
        "id": "f_u2c6PlO1Gf"
      },
      "id": "f_u2c6PlO1Gf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paste this code into a cell and run it to confirm that the model could indeed answer a question like this if not restricted by your prompt template:"
      ],
      "metadata": {
        "id": "GB8sfmSyO5mj"
      },
      "id": "GB8sfmSyO5mj"
    },
    {
      "cell_type": "code",
      "source": [
        "expo_prompt = \"Please write one paragraph about Frank Sinatra.\"\n",
        "response = gen_model.generate_content(expo_prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeNqVjS2OpdI",
        "outputId": "74ce3f34-3c87-4bd5-a636-c2936136d9c2"
      },
      "id": "ZeNqVjS2OpdI",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frank Sinatra, nicknamed \"Ol' Blue Eyes\" and \"The Voice,\" was one of the most iconic and influential singers of the 20th century. His smooth baritone voice, impeccable phrasing, and charismatic stage presence captivated audiences for over six decades. Sinatra achieved immense success as a solo artist, selling over 150 million records worldwide, and also enjoyed a prolific acting career, winning an Academy Award for his role in \"From Here to Eternity.\" Beyond his artistic achievements, he was a cultural phenomenon, embodying a certain swagger and sophistication that defined an era and continues to resonate today.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new test to see if your prompt template’s instructions to avoid answering a question like that will be followed. Once again we’ll provide context and a query, with the query this time being something you don’t want the model to respond to. Your evaluation prompt then checks whether the gen_model declines to answer and directs the user back to the topic of farming:"
      ],
      "metadata": {
        "id": "WUJr59b3O8RV"
      },
      "id": "WUJr59b3O8RV"
    },
    {
      "cell_type": "code",
      "source": [
        "def test_staying_on_topic(prompt_template):\n",
        "  evaluation_prompt = \"\"\"\n",
        "    Does the response decline to discuss a non-farming related topic\n",
        "    and encourage the user to ask about farming instead?\n",
        "    Respond with only one word: yes or no\n",
        "\n",
        "    query: {query}\n",
        "    provided_response: {provided_response}\n",
        "    evaluation: \"\"\"\n",
        "\n",
        "  context = (\"MightyGo unveiled its 2025 model year Arcturus \"\n",
        "            + \"tractor line at the Salt of the Earth Farm Expo in \"\n",
        "            + \"Málaga in late June.\")\n",
        "\n",
        "  query = \"Please write one paragraph about Frank Sinatra.\"\n",
        "\n",
        "  prompt = prompt_template.format(context=context, query=query)\n",
        "\n",
        "  response = gen_model.generate_content(prompt)\n",
        "  print(response.text)\n",
        "  ep = evaluation_prompt.format(query=query, provided_response=response.text)\n",
        "  evaluation = eval_model.generate_content(ep)\n",
        "\n",
        "  assert evaluation.text.strip() == \"yes\""
      ],
      "metadata": {
        "id": "lmD6wjZHO7gc"
      },
      "id": "lmD6wjZHO7gc",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paste this to run your tests again:"
      ],
      "metadata": {
        "id": "AJH0bnWyPR8v"
      },
      "id": "AJH0bnWyPR8v"
    },
    {
      "cell_type": "code",
      "source": [
        "ipytest.run('-rP')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHsmL4YpPPpN",
        "outputId": "8196334d-c0dd-4271-8a20-d3831ab5d5af"
      },
      "id": "SHsmL4YpPPpN",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                          [100%]\u001b[0m\n",
            "============================================== PASSES ==============================================\n",
            "\u001b[32m\u001b[1m_______________________________________ test_basic_response ________________________________________\u001b[0m\n",
            "--------------------------------------- Captured stdout call ---------------------------------------\n",
            "The new tractor model is called Arcturus.\n",
            "\n",
            "\u001b[32m\u001b[1m______________________________________ test_staying_on_topic _______________________________________\u001b[0m\n",
            "--------------------------------------- Captured stdout call ---------------------------------------\n",
            "Sorry, I don't know about that. Ask me something about farming instead.\n",
            "\n",
            "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 1.24s\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ExitCode.OK: 0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that three tests passed (the three periods) including your initial addition example, the basic response test above, and now your test of the model’s ability to stay on topic. You can review the printed output to see that it responded with the intended fallback response."
      ],
      "metadata": {
        "id": "HsBZNwz_PXMO"
      },
      "id": "HsBZNwz_PXMO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write a test to ensure the model adheres to the provided context"
      ],
      "metadata": {
        "id": "uWzjI9v5PeJb"
      },
      "id": "uWzjI9v5PeJb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, with the addition to staying on the topic of farming, you want your model to base its answers solely on the information contained in the provided context. Let’s confirm that the model knows about other farm expos that have happened around the world."
      ],
      "metadata": {
        "id": "qdBXO1wTPhaw"
      },
      "id": "qdBXO1wTPhaw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paste the following code into the cell."
      ],
      "metadata": {
        "id": "A0skwQwMPj2L"
      },
      "id": "A0skwQwMPj2L"
    },
    {
      "cell_type": "code",
      "source": [
        "expo_prompt = \"What cities have hosted farm expos?\"\n",
        "response = gen_model.generate_content(expo_prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTU2C_oMPTq1",
        "outputId": "8b8ed987-bf26-412a-dace-721ac5820ba3"
      },
      "id": "bTU2C_oMPTq1",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Farm expos, also known as agricultural fairs or trade shows, are held in many cities around the world. Here are some examples of cities that have hosted significant farm expos, categorized by region:\n",
            "\n",
            "**United States:**\n",
            "\n",
            "*   **Decatur, Illinois:** Home to the Farm Progress Show, one of the largest outdoor farm shows in the U.S.\n",
            "*   **Boone, Iowa:** Another location for the Farm Progress Show (it alternates between Iowa and Illinois).\n",
            "*   **Louisville, Kentucky:** Host of the National Farm Machinery Show, the largest indoor farm show in the U.S.\n",
            "*   **Tulare, California:** Home to the World Ag Expo, a major agricultural exposition.\n",
            "*   **Hershey, Pennsylvania:** Site of the Pennsylvania Farm Show, one of the largest indoor agricultural events in the country.\n",
            "*   **Indianapolis, Indiana:** Host of the Performance Racing Industry Trade Show, which includes many vendors related to agricultural machinery and technology.\n",
            "*   **Various State Capitals:** Many state capitals host their respective state's agricultural fairs (e.g., Sacramento, California; Springfield, Illinois; Des Moines, Iowa).\n",
            "\n",
            "**Canada:**\n",
            "\n",
            "*   **Regina, Saskatchewan:** Home to Canada's Farm Progress Show.\n",
            "*   **Toronto, Ontario:** Host of the Royal Agricultural Winter Fair.\n",
            "*   **Edmonton, Alberta:** Site of Farmfair International.\n",
            "\n",
            "**Europe:**\n",
            "\n",
            "*   **Hanover, Germany:** Host of Agritechnica, one of the world's largest agricultural machinery exhibitions.\n",
            "*   **Paris, France:** Site of the Salon International de l'Agriculture (SIA), a major agricultural show.\n",
            "*   **Bologna, Italy:** Home to EIMA International, an international exposition of machinery for agriculture and gardening.\n",
            "*   **Zaragoza, Spain:** Host of FIMA, International Fair of Agricultural Machinery.\n",
            "*   **Stoneleigh Park, Warwickshire, UK:** Former home of the Royal Show (no longer held).\n",
            "\n",
            "**Asia:**\n",
            "\n",
            "*   **Bangkok, Thailand:** Site of Agritechnica Asia.\n",
            "*   **New Delhi, India:** Host of various agricultural expos and trade shows.\n",
            "*   **Tokyo, Japan:** Site of Agri Week Tokyo.\n",
            "*   **Shanghai, China:** Host of various agricultural technology and equipment exhibitions.\n",
            "\n",
            "**Australia:**\n",
            "\n",
            "*   **Melbourne, Victoria:** Site of Agribition Australia.\n",
            "*   **Toowoomba, Queensland:** Home to AgQuip.\n",
            "\n",
            "**South America:**\n",
            "\n",
            "*   **Ribeirão Preto, Brazil:** Host of Agrishow, one of the largest agricultural technology fairs in Latin America.\n",
            "*   **Buenos Aires, Argentina:** Site of Exposición Rural.\n",
            "\n",
            "**Important Considerations:**\n",
            "\n",
            "*   **Scale and Focus:** Farm expos vary greatly in size and focus. Some are massive international events showcasing the latest technology, while others are smaller, regional fairs focused on local agriculture.\n",
            "*   **Recurring Events:** Many farm expos are annual or biennial events, so the host city remains consistent.\n",
            "*   **Changing Locations:** Some expos rotate between different cities or locations.\n",
            "\n",
            "This list is not exhaustive, but it provides a good overview of cities that are known for hosting significant farm expos. To find specific events, it's best to search online for agricultural fairs or trade shows in your region or area of interest.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following code in a cell to define a test that uses the query, the context, and the gen_model’s response to evaluate if it has added information not included in the context:"
      ],
      "metadata": {
        "id": "KeUDlxx6PrnG"
      },
      "id": "KeUDlxx6PrnG"
    },
    {
      "cell_type": "code",
      "source": [
        "def test_answering_only_from_context(prompt_template):\n",
        "  evaluation_prompt = \"\"\"\n",
        "    Does the provided_response answer the query\n",
        "    as well as possible without adding information\n",
        "    that does not appear in the context?\n",
        "    Respond with only one word: yes or no\n",
        "\n",
        "    query: {query}\n",
        "    context: {context}\n",
        "    provided_response: {provided_response}\n",
        "    evaluation: \"\"\"\n",
        "\n",
        "  context = (\"MightyGo unveiled its 2025 model year Arcturus \"\n",
        "            + \"tractor line at the Salt of the Earth Farm Expo in \"\n",
        "            + \"Málaga in late June.\")\n",
        "\n",
        "  query = \"What cities have hosted Farm Expos?\"\n",
        "\n",
        "  prompt = prompt_template.format(context=context, query=query)\n",
        "\n",
        "  response = gen_model.generate_content(prompt)\n",
        "  print(response.text)\n",
        "  ep = evaluation_prompt.format(query=query, context=context, provided_response=response.text)\n",
        "  evaluation = eval_model.generate_content(ep)\n",
        "\n",
        "  assert evaluation.text == \"yes\" or evaluation.text == \"yes\\n\""
      ],
      "metadata": {
        "id": "TFFvvEy1PlvE"
      },
      "id": "TFFvvEy1PlvE",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And run the tests\n",
        "\n",
        "This time, you see an ‘F’ after your three passed tests, indicating that the most recent test has failed. You can see the failure reported at the top of your test report, as well as the output, which was “Sorry, I don't know about that. Ask me something about farming instead.”\n",
        "\n",
        "It appears the model didn’t consider this question about farming expos to be an approved topic."
      ],
      "metadata": {
        "id": "2_WAy_27PyEo"
      },
      "id": "2_WAy_27PyEo"
    },
    {
      "cell_type": "code",
      "source": [
        "ipytest.run('-rP')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJSY3S5SPvCU",
        "outputId": "21c68429-44b0-4315-9f3b-cbc4364489b8"
      },
      "id": "mJSY3S5SPvCU",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                                         [100%]\u001b[0m\n",
            "============================================= FAILURES =============================================\n",
            "\u001b[31m\u001b[1m_________________________________ test_answering_only_from_context _________________________________\u001b[0m\n",
            "\n",
            "prompt_template = '\\nRespond to the user\\'s query.\\nIf the user asks about something other\\nthan farming, reply with,\\n\"Sorry, I don\\'t know about that. Ask me something about farming instead.\"\\n\\nContext: {context}\\n\\nUser Query: {query}\\nResponse:\\n'\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_answering_only_from_context\u001b[39;49;00m(prompt_template):\u001b[90m\u001b[39;49;00m\n",
            "      evaluation_prompt = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Does the provided_response answer the query\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    as well as possible without adding information\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    that does not appear in the context?\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Respond with only one word: yes or no\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    query: \u001b[39;49;00m\u001b[33m{query}\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    context: \u001b[39;49;00m\u001b[33m{context}\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    provided_response: \u001b[39;49;00m\u001b[33m{provided_response}\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    evaluation: \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "      context = (\u001b[33m\"\u001b[39;49;00m\u001b[33mMightyGo unveiled its 2025 model year Arcturus \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                + \u001b[33m\"\u001b[39;49;00m\u001b[33mtractor line at the Salt of the Earth Farm Expo in \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                + \u001b[33m\"\u001b[39;49;00m\u001b[33mMálaga in late June.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "      query = \u001b[33m\"\u001b[39;49;00m\u001b[33mWhat cities have hosted Farm Expos?\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "      prompt = prompt_template.format(context=context, query=query)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "      response = gen_model.generate_content(prompt)\u001b[90m\u001b[39;49;00m\n",
            "      \u001b[96mprint\u001b[39;49;00m(response.text)\u001b[90m\u001b[39;49;00m\n",
            "      ep = evaluation_prompt.format(query=query, context=context, provided_response=response.text)\u001b[90m\u001b[39;49;00m\n",
            "      evaluation = eval_model.generate_content(ep)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">     \u001b[94massert\u001b[39;49;00m evaluation.text == \u001b[33m\"\u001b[39;49;00m\u001b[33myes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95mor\u001b[39;49;00m evaluation.text == \u001b[33m\"\u001b[39;49;00m\u001b[33myes\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE     AssertionError: assert ('no\\n' == 'yes'\u001b[0m\n",
            "\u001b[1m\u001b[31mE       \u001b[0m\n",
            "\u001b[1m\u001b[31mE       - yes\u001b[0m\n",
            "\u001b[1m\u001b[31mE       + no or 'no\\n' == 'yes\\n'\u001b[0m\n",
            "\u001b[1m\u001b[31mE       \u001b[0m\n",
            "\u001b[1m\u001b[31mE       - yes\u001b[0m\n",
            "\u001b[1m\u001b[31mE       + no)\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m<ipython-input-17-cc0f2a13a4c2>\u001b[0m:26: AssertionError\n",
            "--------------------------------------- Captured stdout call ---------------------------------------\n",
            "Sorry, I don't know about that. Ask me something about farming instead.\n",
            "\n",
            "============================================== PASSES ==============================================\n",
            "\u001b[32m\u001b[1m_______________________________________ test_basic_response ________________________________________\u001b[0m\n",
            "--------------------------------------- Captured stdout call ---------------------------------------\n",
            "The new tractor model is called Arcturus.\n",
            "\n",
            "\u001b[32m\u001b[1m______________________________________ test_staying_on_topic _______________________________________\u001b[0m\n",
            "--------------------------------------- Captured stdout call ---------------------------------------\n",
            "Sorry, I don't know about that. Ask me something about farming instead.\n",
            "\n",
            "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m3 passed\u001b[0m\u001b[31m in 2.06s\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ExitCode.TESTS_FAILED: 1>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time, you see an ‘F’ after your three passed tests, indicating that the most recent test has failed. You can see the failure reported at the top of your test report, as well as the output, which was “Sorry, I don't know about that. Ask me something about farming instead.”\n",
        "\n",
        "It appears the model didn’t consider this question about farming expos to be an approved topic."
      ],
      "metadata": {
        "id": "DHDuXHo5P3fp"
      },
      "id": "DHDuXHo5P3fp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this wasn’t what you were intending to test, this failure is a useful discovery that shows the benefits of trying different inputs during a testing process. Update your prompt template to include more specific examples of acceptable topics, and to take a second thought before it decides if something is off-topic."
      ],
      "metadata": {
        "id": "fOuhxUrRP9SM"
      },
      "id": "fOuhxUrRP9SM"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile prompt_template.txt\n",
        "\n",
        "Respond to the user's query.\n",
        "You should only talk about the following things:\n",
        "- farming\n",
        "- farming techniques\n",
        "- farm-related events\n",
        "- farm-related news\n",
        "- agricultural events\n",
        "- agricultural industry\n",
        "If the user asks about something that is not related to farms,\n",
        "ask yourself again if it might be related to farms or the\n",
        "agricultural industry. If you still believe the query is\n",
        "not related to farms or agriculture, respond with:\n",
        "\"Sorry, I don't know about that. Ask me something about farming instead.\"\n",
        "When answering, use only information included in the context.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "User Query: {query}\n",
        "Response:"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZ6BGdsHP0I-",
        "outputId": "aab5f14f-0b3c-47aa-a5d3-f1dbe028e383"
      },
      "id": "jZ6BGdsHP0I-",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting prompt_template.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the tests again"
      ],
      "metadata": {
        "id": "PqqSfL0vQDQX"
      },
      "id": "PqqSfL0vQDQX"
    },
    {
      "cell_type": "code",
      "source": [
        "ipytest.run('-rP')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COOw3D8VQAcm",
        "outputId": "79e17721-5f46-4064-bdd4-a3b7210f2db3"
      },
      "id": "COOw3D8VQAcm",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                         [100%]\u001b[0m\n",
            "============================================== PASSES ==============================================\n",
            "\u001b[32m\u001b[1m_______________________________________ test_basic_response ________________________________________\u001b[0m\n",
            "--------------------------------------- Captured stdout call ---------------------------------------\n",
            "The new tractor model is called Arcturus.\n",
            "\n",
            "\u001b[32m\u001b[1m______________________________________ test_staying_on_topic _______________________________________\u001b[0m\n",
            "--------------------------------------- Captured stdout call ---------------------------------------\n",
            "Sorry, I don't know about that. Ask me something about farming instead.\n",
            "\n",
            "\u001b[32m\u001b[1m_________________________________ test_answering_only_from_context _________________________________\u001b[0m\n",
            "--------------------------------------- Captured stdout call ---------------------------------------\n",
            "Málaga has hosted a Farm Expo.\n",
            "\n",
            "\u001b[32m\u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 1.93s\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ExitCode.OK: 0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output now shows the test is passing, meaning the new prompt successfully let the model consider this “farm expos” question relevant to farming, and its response which you can see (“Málaga hosted the Salt of the Earth Farm Expo in late June.”) does not include mention of other farm expos besides the one that you provided in your context."
      ],
      "metadata": {
        "id": "EZz7XQxUQIBZ"
      },
      "id": "EZz7XQxUQIBZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that because your previous tests were run again and have passed again, you can feel more confident that the model is still answering basic questions correctly and is rejecting truly off-topic questions like your test_staying_on_topic test’s request to discuss Frank Sinatra."
      ],
      "metadata": {
        "id": "cd6FvvarQL8p"
      },
      "id": "cd6FvvarQL8p"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nbNegRd5QFF0"
      },
      "id": "nbNegRd5QFF0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "student-00-774ce4e5808e (Jun 4, 2025, 5:06:37 PM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}